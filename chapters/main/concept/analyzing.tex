\section{Analyzing process}
\label{ch:theory:analyzing_process}
At this point the secrets to be discovered are defined and the file access method to be used is determined. 
The next step in this concept is to define an analyzing process in order to detect these mentioned secrets.
The analyzing module contains 4 sub-modules.
Each sub-module works independent to provide most of flexibility.
\begin{itemize}
\item Image-obtaining
\item Meta-extraction
\item Image-mount
\item File-scan
\end{itemize}
The structure of this analyzing section goes as followed.
First the abstractive flowchart in Figure \ref{ch:theory:analyzing process:fig} gives basic informations about the analyzing process in general. 
It contains the analysis workflow shown with every module in action. 
Afterwards each module is described in detail in a separate subsection.
Finally there is a pseudocode in Listing \ref{ch:theory:analyzing_process:pseudofig} that shows the analyzing workflow in one comprehensive flow to catch the last detail questions.

Figure \ref{ch:theory:analyzing process:fig} starts with obtaining the target image. 
This obtaining module needs an input argument which is the name of the Docker image.
The obtaining module deletes old Docker images and downloads the target image from a container registry. 
Then the preprocessing module extracts the metadata informations from the image. 
If the processor recognizes the keywords COPY, ADD or RUN a further analysis of the image is necessary.
These three commands are responsible for adding possible secrets statically into the image.
No further image investigation will take place when none of these keywords exists. 
It is important to note that a RUN keyword does not appear directly in the meta informations. 
Instead the RUN command is represented by a list of program names that have to be compared with the meta-data.
The metadata-informations are already explained in section \ref{sec:intro:docker_image:docker_img:meta}.
If the analysis had been taken place the image would have been mounted by the image-mount module.
Necessary mounting informations are already available as mentioned in section \ref{sec:intro:docker_image:docker_img:architecture}.
Finally the scan of RSA keys and Amazon access token takes place with the mentioned methods from chapter \ref{ch:known_key_leak_techniques}, namely keyword-based search and pattern-based search. A result is then returned in a flexible way.
The architecture itself has to be built in a way of reusability and should work in a heterogeneous environment.
This is achieved through development with proven properties from the field of distributed systems. 
This includes working with standardized structures and scalable microservices.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{gfx/examples/basic_workflow_scan}
\caption{Abstract flowchart of the analyzing process}
\label{ch:theory:analyzing process:fig}
\end{figure}

\subsection{Image-obtaining}
\label{ch:theory:analyzing_process:obtain}
The architecture of this module belongs to a classical input output system. This module takes the image name as an argument and will download the image as a file output on the host system. 
As already known, the output is saved as several folders and files depending on the used union file system which represents the image. 
Before the download starts the module has to take care about garbage of old images in order to save hardware resources and prevent errors during the future proceeding. 
This has to be done through the Docker API since Docker already provides functions to clean up images from a host system.

The target image can be downloaded from an arbitrary container registry.
In this work the module only pulls images from public registries without authentication. 
In theory it is also possible to pull images from registries where authentication is needed. 
These actions would also take place in this module.
If the image does not exist the analyzing process is terminated.
However if this module has found and downloaded an image it continues with the preprocessing of the image. 
The responsible module is explained in the next subsection.

\subsection{Image preprocessing}
\label{ch:theory:analyzing_process:prepro}
This following module is very important as it forms the basis for the file scan module. 
It primarily decides whether the specified image is in need of a scan.
The module can also decide on which part is in need of a scan for embedded secrets. 
The basis for this decision builds the meta-information check. 
As written in the background chapter the history of an image provides several meta informations like the executed Dockerfile commands.
If keywords such as ADD and COPY are found or any RUN methods are used further investigation or analysis is required.
There is no suspicion of secrets being integrated and the termination process can be initiated if none of these have been performed.
The reason to check only COPY, ADD and RUN keywords is described in section \ref{sec:intro:docker_image:docker_img:meta}. 

The module image preprocessing has to handle the Dockerfile actions ADD/COPY and RUN differently in case of further investigation as well.
In the matter of the ADD and COPY command there is a resulting pattern in the meta information that always seems to be the same.
The pattern is the following.
\begin{lstlisting}
ADD file:08e718ed0796013f5957a1be7da3bef6225f3d82d8be0a86a7114e5caad50cbc in /
\end{lstlisting}
It can be seen that an ADD follows a \textit{file}, separated by colon and followed by HEX code with 65 signs. 
It is also noticable that the destination folder appears after the HEX code.
Especially the destination path is interesting to watch. 
The preprocessing module has to find this destination. 
Therefore a corresponding pattern like the following might help.
\begin{lstlisting}
"(dir|file):[a-f0-9]{64}\sin\s"
\end{lstlisting}
ADD can be treated analogous to COPY because of the same syntax in the Dockerfile.

Recognized destination paths finally set the folders that the scan-module has to examine with help of the introduced pattern. 
These folders are the potential target paths where secrets may exist.
Due to the flexibility of these COPY and ADD commands there are a few cases to consider in order to examine the correct targets.
It is important to note that many different folders can be examined folders with the same name.
Different folders without a file system relation are not a complication and can be therefore treated as a normal scanning target. 
If duplicate folders are detected the preprocessor must detect and remove this duplicate so that the folder is considered only once.
%If a child parent relationship exists, the child folder can be removed and only the parent folder forms the root directory to be scanned. The parent folder contains the subfolder and the child is considered only once.
%A possible relation of folders can be recognized by a corresponding comparison of the given path specifications. The analyzing module has to take care of that with implementing such a decision. 

Subsequently the RUN command implies the simple bash command which already exists in each metadata entry in the history. 
This is the reason why RUN is not explicitly listed as a keyword in the metadata informations. 
One approach is to detect actions that follow a RUN action as they are listed in the metadata information. 
That means there is a data structure with valid keywords necessary which will be compared to the whole metadata.
The following enumeration shows commands which must be inserted into a suitable data structure for valid comparison with the metadata.
\begin{itemize}
\item ssh-keygen
\item openssl
\item git clone
\item wget
\end{itemize}
This list is not complete but it provides a first step to extract useful informations from originally RUN commands. 
All of these programs are ubiquitous.
Ssh-keygen and openssl are able to create many types of keys and most of all RSA key pairs. 
In this context the private part is the interesting part which can be looked further into.
Wget and git clone are common utils to request and download a bunch of files, folder and archives from an endpoint. 
Especially git clone is ubiquitous since git is very common for developers.
Furthermore it is important to recognize these particular programs only when they have been used and not installed.
A developer might install openssl via a package manager like apt when openssl is not natively available.
This installation does not lead to a integration of secrets and has to be omitted.
Furthermore a concatenation of tools is possible and has to be considered by this module. 
The concatenation looks exemplarely like the following.
\begin{lstlisting}
	git clone https://github.com/blackbird71SR/Hello-World && wget https://lorempixel/secret.jpg
\end{lstlisting}

These requirements lead to a potentially valid pattern derived from the metadata information.
\begin{lstlisting}
	"(/bin/sh\s-c|&&)\s(openssl genrsa|wget|git clone|ssh-keygen)"
\end{lstlisting}

For each detected program the syntax must be taken into account. 
Each program specifies an optional target path that must be extracted. 
These form the next potential targets which must be considered by the scanning module. 
Special interrelation between folders like duplications have also to be considered when the targets are examined.

If no optional endpoint is specified for a program, the current WORKDIR is used. This belongs to the COPY or ADD and to the RUN command.
\graffito{The WORKDIR instruction sets the working directory for any RUN, COPY and ADD command in the Dockerfile}
The WORKDIR variable is set to the root(/) folder on the target system if no other specification has been made by the developer. 
It must also be ensured that the correct relative base path is considered when the WORKDIR keyword is used. 
The following snippet shows the effect of using the WORKDIR variable:
\begin{lstlisting}
	ADD test relativeDir/          # adds "test" to `WORKDIR`/relativeDir/
	ADD test /absoluteDir/         # adds "test" to /absoluteDir/
\end{lstlisting}
It can be seen that the WORKDIR variable is simply used as prefix when a relative Unix path is used.
If the target path of an ADD, COPY and RUN command is recognized, the WORKDIR variable must finally be taken into account unless an absolute path was used.
However it is a special use case when the WORKDIR variable is set as root directory or the developer used an instruction to copy files to the root folder. 
In this case everything should be scanned except for the default root files and folders.
This behavior might be confusing and is demonstrated with the following listing.
Before the final targets can be defined by the preprocessing module, the root files and folders of a plain Linux system has to be set. 
This can be done static and scalable since there are standards of root hierchachies established.
Every folder that is not listed from the static root folder deviates from the original file system and sets a certain difference. 
This difference finally sets the target path(s) to scan. 

In general this scanning of certain areas enables a higher analysis speed and reduces the false-positive rate significantly.
Unfortunately this advantage is also a disadvantage at the same time. 
Secrets from upper base images can not be discovered because associated meta informations are only available starting from the latest base image.
That requires an already trusted base image.
These base images are marked on container registries as official or trusted. 
It is a must do to only use trustworthy base images since the key problems are not solely existing. 
Also other types of security vulnerabilities may be included such as manipulated application packages. 
It is also important to note that no manipulation on the history can be recognized with this module. 
This would be an important function to extend otherwise the manipulation would allow to bypass this preprocessing module. 

However this work belongs to normal development processes without manipulations of a build pipeline.
This preprocessing module starts at the point where a developer or system engineer creates its own layer from a trusted base image.
Finally the targets to be scanned must be made available when the preprocessing is successfully done. 
The image-mount module is responsible for that and will be explained in the following.

\subsection{Image-mount}
\label{ch:theory:analyzing_process:imgmount}
In the current state the file system only contains folders of layers that belong to the selected image.
Due to the nature of images it is not possible to map the determined folders to the layers on file system level. 
In other words there is no assignment of the identified folders to be scanned to the really existing folders on the file system.
This is an essential element since this module mainly takes care of providing the real data. 
This in fact would have reduced the number of folders in the folder chain to mount. 

Due to this reason the ultimate goal of this module is to provide access to all folders of the image for the scan module. 
Since Docker creates an overlay over the folders this module will achieve this as well. 
This overlay leads to an access point for the scanning module. 
How lower directories are chained has to be be examined through the local provided informations of the image.
This chain is the base to create an overlay mount point.
As well a merged folder, a Working folder for the union file system and a \textit{writeable} folder needs to be provided accordingly.
The according elements of an Overlay2 file system are described in section \ref{sec:intro:docker_image:unionfs}.
If the mount process was successful the scan module is triggered in order to detect potential secrets. 
This module will be explained in the following.

\subsection{File-scan}
\label{ch:theory:analyzing_process:scan}
The file scan module is responsible for analyzing specific files for secrets. 
It is known that this module must detect RSA private keys and Amazon AWS tokens since it is the goal of this Bachelor thesis.

In case of the AWS tokens the amount of existing programming languages make it difficult to include all technologies in a prototype. 
The idea is to rather start with specific files to get exactly comprehensible results. 
This means that not every type of source code can be examined immediately and this module must be built scalable to support more environments in the future.
To be more specific the module will not strive for archives and binaries but will instead refer to pure source code files.
In order to find the Amazon AWS token the module will check only specific source code.
An Amazon access token is based on a fixed scheme that must be used by this scanning engine.
The pattern looks like the following.
\begin{lstlisting}
AKIA[0-9A-Z]{16}
\end{lstlisting}

For the recognition of RSA private keys it is not clear what the name and the file extension of the token is.
This is the reason why every file in the given directory hierarchy will be checked.
The file scan module will search for this fixed prefix at context level in the respective folder hierarchy.
\begin{lstlisting}
-----BEGIN OPENSSH PRIVATE KEY-----
\end{lstlisting}
\begin{lstlisting}
-----BEGIN RSA PRIVATE KEY-----
\end{lstlisting}
\begin{lstlisting}
-----BEGIN PRIVATE KEY-----
\end{lstlisting}

In both cases the analysis must be done recursively because there can be another folder within a folder which therefore can be nested other folders again.
The analysis is performed programmatically with native Linux tools and with file operations to browse through files.
When the module determines a hit the result is beeing added to a data structure.
At the end of the scan the result is returned to the requesting client.

To finish off the analyzing process Figure \ref{ch:theory:analyzing_pseudocode} shows a more detailed sequence of analysis.
It is a pure logical sequence without going into technical details and implementations. 

\subsection{Pseudo code analyzing}
\label{ch:theory:analyzing_pseudocode}
The following pseudocode in Listing \ref{ch:theory:analyzing_process:pseudofig} is aligned to the programming language Python because it omits all the distracting brackets and makes it more readable to English speakers/readers.
For completeness the pseudo code contains a short description of what the program does for a task. 
Afterwards the abstract algorithm is started.
\lstinputlisting[caption={Pseudocode - analyzing workflow}, captionpos=b, label={ch:theory:analyzing_process:pseudofig}]{chapters/main/concept/listings/pseudocode.txt}
This chapter provided the theoretical concept to detect embedded secrets in Docker images. The next chapter concentrates on the practical realization.
