\section{Implementation core modules}
\label{ch:practical_realization:implementation}
This section demonstrates the prototypical realization of the main modules, namely obtain, preprocessing, mount, scan and their interaction through the associated manager. Each module is dedicated to one section. The order of the modules to be described is based on the sequence of the analysis process from the theory chapter \ref{ch:theory:analysing_process}.
The prototypical realization of each module is shown with an overview of the methods used in combination with a practical implementation meaning code snippets in order to archive the desired goals. 

First it starts with the obaint module.

\subsection{obtain.py}
\label{ch:practical_realization:implementation:obtain}
The obtaining module has to take care about garbage of the environment and pulling the target image.
This module is build very slim and contains only a few methods.
\begin{itemize}
\item stop\_all\_containers(self)
\item remove\_old\_containers(self)
\item pull\_image(self)
\end{itemize}
The procedure consists of two steps, which are represented by the methods stop\_all\_containers(self) and remove\_old\_containers(self). First, possible running containers needs to be stopped and afterwards all locally existing images and therefore all overlay directories on the filesystem has to be removed. This is programmatically done through the equivalent Docker stop and Docker remove command as shown in the code-snippet \ref{ch:practical_realization:implementation:obtain:code}. These commands are accessible through the available Docker SDK, which has to be installed and integrated into the virtual environment before. 
The code-snippet itself is explanation enough. It is just important to note that the container.reload() method is used to get all valid attributes of each container in order to stop it correctly afterwards.

Second, the Docker pull command is simply used to download the target image. The module expects a string as an argument, which tries to download the image with the given name.
If no tag is specified, the latest tag of the image will be used. This is achieved by simple string manipulation since it is known that the image name and tag are separated by colon as described in \ref{sec:intro:docker_image:docker_img:architecture}.
It is necessary to specify an image tag for the download. If no tag is specified every image will be downloaded with all available tags. The corresponding code snippet to this function can be seen in the code-snippet \ref{ch:practical_realization:implementation:obtain:code} as well.
\lstinputlisting[language=Python,caption={Python snippet - obtaining image}, captionpos=b,label={ch:practical_realization:implementation:obtain:code}]{chapters/main/practical/listings/code/garbage.py}

\subsection{preprocessing.py}
\label{ch:practical_realization:implementation:preprocessing}
The pre-processing module decides whether an image needs to be scanned and if so, it decides which parts need to be scanned. This preprocessing reduces the amount of false positives and increases a much faster pace when it comes to scanning.
This corresponding python module needs a bit more logic to work properly. To get an overview of the logic, the following function list helps. Only core functions are listed, helper functions are not listed as they are not necessary at this point.
\begin{itemize}
\item collect\_metadata(self)
\item contains\_key\_actions(self)
\item fetch\_direct\_copy\_targets(self)
\item fetch\_indirect\_copy\_targets(self)
\item clearup\_targets(self)
\item examine\_workdir(self)
\end{itemize}

The collect\_metadata function is the first important method. It initilizes the Docker environment in order to fetch the locally provided target image. Afterwards the extraction of this fetched image takes place. This is done through the equivalent Docker history command from the Docker SDK. The history provides metadata informations, which are explained in \ref{sec:intro:docker_image:docker_img:meta}. This metadata will be saved in an instance attribute to provide a global access to these metadata.

The decision whether the target image has to be scanned or not is made in the contains\_key\_actions method. 
As developed in the concept in \ref{ch:theory:analysing_process:prepro}, an image has to be scanned when keywords such as ADD, COPY or any RUN commands have been used during the building process.
This can be determined with help of a proper data-structure compared against the metadata information of the image.
That data-structure includes proper keywords and an additional related variable if this keyword has been used.
The Python dictionary is a data-structure to provide one or more key:value pairs. The key:value pair represents the keyword with a status respectively.
The key part of the data-structure is derived from the concept. The value of each key is set by default to False. The final data-structure is shown below.
\lstset{language=Python}          % Set your language (you can change the language for each code-block optionally)
\begin{lstlisting}[]  % Start your code-block
	
    action_dict = {
        "ADD": False,
        "COPY": False,
        "openssl": False,
        "wget": False,
        "git clone": False,
        "ssh-keygen": False,
    }
    
\end{lstlisting}
The comparison implemented in Python can be seen in \ref{ch:practical_realization:implementation:preprocessing:code_contains}. When an entry from the dictionary is detected, the corresponding value will be updated to true.
Finally the whole data\_structure will be checked and a boolean is set and returned. The method contains\_key\_actions(self) returns true if any key values are set to true. That means in general that a further investigation of the image is mandatory. Only when every value is still false, the boolean is set to False too and no further investigation is necessary.
\lstinputlisting[language=Python,caption={Python snippet - scanning decision}, captionpos=b,label={ch:practical_realization:implementation:preprocessing:code_contains}]{chapters/main/practical/listings/code/contains.py}

The next important functions of this preprocessing module are fetch\_direct\_copy\_targets and fetch\_indirect\_copy\_targets. Both functions have in common to extract and to return detected path(s) used by COPY, ADD or RUN. The direct copy function takes care about fetching targets which has been integrated via ADD and COPY. The indirect methods takes care about the indirect integration via openssl, wget, git clone and ssh-keygen. The implementation of both methods looks different.
The method for direct copying searches in the meta information for this following regex pattern which was determined in the theoretical concept \ref{ch:theory:analysing_process:prepro}.
\begin{lstlisting}
"(dir|file):[a-f0-9]{64}\sin\s"
\end{lstlisting}
If this pattern matches in the meta information, a string slicing takes place in order to extract the target path. Simple string slicing with determination of the position of special delimiter signs is possible. This belongs to the fixed syntax in the meta information. The syntax is readable because of the JSON similarity.
The examination of the target path realised in Python can be seen in the code-snippet below.
This developed algorithm deserves a tiny explanation.
\lstinputlisting[caption={Python pseudo snippet - fetch COPY/ADD targets}, captionpos=b,label={ch:practical_realization:implementation:preprocessing:code_fetchdirect}]{chapters/main/practical/listings/code/fetchdirect.txt}
In principle, each match from the Regex pattern is processed further. A list forms the data structure to which the determining target folders are appended. The determination starts with examining the target\_path. 
When the target\_path is the root directory, the next steps are omitted and the next round of the loop starts. This belongs only to the last entry, which is always an ADD command for the base image layer. This has no effect on a copy action from a developer who explicitly chose the root directory as target.

When the loop is continuing normally, a temporary metadata is extracted in order to set the current WORKDIR for the corresponding target\_path. Since the WORKDIR variable can be set more often in the build-process and can therefore occur more often in the metadata, the WORKDIR variable must be determined exactly. This is why a string slicing is necessary. The slicing takes places from the start of the original metadata until the position of the already determined corresponding target directory. 
The last occurring WORKDIR variable of this sliced metadata sets the final WORKDIR variable for the target\_path. The WORKDIR variable is important when the target\_path is relativ instead of absolute as known from \ref{h:practical_realization:implementation:preprocessing}.

After setting this WORKDIR it is important to distinguish between the root path, relativ paths and absolute paths. This can be examined as seen in the pseudo snippet with analysing the first character of the already known target\_path.
When the root is recognised, a simple adding of '/.' to the target\_list is made. If a relative path is recognized a concatenation will take place in order to set the correct target folder. One exception is the point, which represents just the examined WORKDIR variable. In this case the WORK is the target directory. 
Finally the full examined target directory to scan is appended to the list. The array is returned after this processing.

As already known fetch\_indirect\_copy\_targets(self) searches in the meta information for special programs. These programs are recognized in a regex pattern which was defined in the theoretical concept \ref{ch:theory:analysing_process:prepro}.
\begin{lstlisting}
"(/bin/sh\s-c|&&)\s(openssl genrsa|wget|git clone|ssh-keygen)"
\end{lstlisting}
When any of these commands are recognized, a helper function to this associated command is called. The helper function is mainly responsible for the string slicing.
It expects the output option of the related tool and the current index of the option, where the option has been found. The output option may differ depending on the command. Thats is why a helper function comes in place which is called by the parent method. The helper function will determine and return the output path finally.
The helper function may be called multiple times, depending of the amount of matches in the meta-data. That is why an examined target from the helper function is appended to an array. 
A special case takes place when no output argument is given. In that case the current WORKDIR has to be checked and returned. As already known, this applies to COPY and ADD as well and will be handled in this method RUN in the same way. Just as with COPY and ADD, absolute and relative paths play a role here. These are treated identically, because they all have the same goal.
A full in depth insight of the corresponding work between the fetch\_indirect\_copy\_targets(self) and the helper functions can be seen in source code of the attached CD.

Finally this preprocessing module holds a clearup\_targets method. This method takes care about the data structure which holds the targets globally. The global data structure is transformed in a Python set. This set automatically removes duplicates.
	
At this point the analysing\_handler module can trigger to pull an image and manage that preprocessing of that image. The target directories are examined and the necessary mount module can do its job. The realization of this mount module is described in the next subsection.

\subsection{mount.py}	
\label{ch:practical_realization:implementation:mount}
The structure of the mount module is built straight forward. Only core functions are listed below:
\begin{itemize}
\item create\_overlay\_dirs(self)
\item remove\_overlay\_dirs(self)
\item mount\_overlay(self)
\item unmount\_overlay(self)
\end{itemize}
The functions create\_overlay\_dirs and remove\_overlay\_dirs are responsible for creating the working directory environment. As written in \ref{sec:intro:docker_image:unionfs} there are a couple of working directories necessary to mount an Overlay2 file system.
Every folder in except of the lower directories, which represents the image itself will be created or deleted. The creation and deletion is programmatically done via the available os package and is not worth to show at this point, since it is just a creation and delete process triggered. It is important to note that the lower directories are not created or deleted. The lower directories are automatically deleted when the obtaining module starts the cleaning up the garbage.
 
The method mount\_overlay first examines the lower directories of the overlay hierarchy. When these are examined, every information to mount an overlay is provided.
The examination of the lower directories is done by a helper function.
As known from the background chapter, every Docker image-layer is connected through a lowerfile.
The lowerfile of the highest level of the image contains the most lower directories. 
The main task of the helper function is to determines this lowerfile.
An additional Python package called "path" helps to walk through the local provided Docker layers in order to provide access to all lowerfiles. The helper function returns finally the content of the lowerfile of the highest layer of the Docker image.
The examination is shown in the code-snippet \ref{ch:practical_realization:implementation:preprocessing:code_getlowerdirs} below and deserves an explanation.

\lstinputlisting[language=Python,caption={Python snippet - get lowerdirs}, captionpos=b,label={ch:practical_realization:implementation:preprocessing:code_getlowerdirs}]{chapters/main/practical/listings/code/getlowerdirs.py}
Initially an empty array is created. Afterwards a loop goes over every Docker image layer on the filesystem, which is represented by the overlay\_path variable.
In each folder exists this lowerfile. This lowerfile will opened in read-only mode and the raw content finally appended to the array. When the for loop is finished, a quantity calculation of the characters in each array entry can be performed. The entry with the most characters represents the final lowerfile.	
If this is examined and the content returned, the information of the lowerfile can be directly used as a parameter for the mounting process after by the method mount\_overlay(self).
The mount command can be finally by a system call which looks like below.
\lstinputlisting[language=Python,caption={Python snippet - mount}, captionpos=b,label={ch:practical_realization:implementation:preprocessing:code_mount}]{chapters/main/practical/listings/code/mount.py}
It can be seen that remove\_overlay\_dirs(self) and create\_overlay\_dirs(self) are triggered first. Afterwards a helper function to get a correct order of lower directories is called. With that gained information the actual mount process can be executed. The name of that overlay\_name variable is important since it is used to unmount the process like the following snippet \ref{ch:practical_realization:implementation:preprocessing:code_umount} shows.
\lstinputlisting[language=Python,caption={Python snippet - umount}, captionpos=b,label={ch:practical_realization:implementation:preprocessing:code_umount}]{chapters/main/practical/listings/code/umount.py}

Finally the overlay is created and a direct access to the image is achieved. This access will be used by the file scan module. The implementation of that module is shown in the next subsection.

\subsection{scan.py}
\label{ch:practical_realization:implementation:scan}
Finally the scan module holds three important functions, excluding constructor method:
\begin{itemize}
\item scan\_for\_rsa\_pk(self)
\item scan\_for\_aws\_key(self)
\item get\_root\_diff(self)
\end{itemize}
The scan\_for\_rsa\_pk(self) function includes a data\_structure to hold prefixes of RSA private keys.
These static prefixes are known from the theory chapter and is seen in the following.
\lstset{language=Python}          % Set your language (you can change the language for each code-block optionally)
\begin{lstlisting}[]  % Start your code-block
	
    Prefix = [
    	"-----BEGIN OPENSSH PRIVATE KEY-----",
	"-----BEGIN RSA PRIVATE KEY-----",
	"-----BEGIN PRIVATE KEY-----"
	]
\end{lstlisting}
The idea behind this scan is to combine each prefix with the already determined target path list.
In the prototype, two nested for-loops serve as the basis for this. The prefix and the element list form the cartesian product.
This information can then be combined with standard linux tools to perform a scan for RSA private keys.
The following code snippet shows the core method to scan these directories with the linux standard utils.

\lstset{language=Python}          % Set your language (you can change the language for each code-block optionally)
\begin{lstlisting}[]  % Start your code-block
	

for prefix in fix_strings:
            for dir in self.target_list:
                if dir is '/':
                    target_root = self.get_root_diff()
                    os.system(f"find {mount.overlay_mount_path + '/' target_root} -type f -iname '*' -exec grep -HIr -- '{prefix}' "'{}'" \;")
                else:
                    os.system(f"find {mount.overlay_mount_path + '/' + dir} -type f -iname '*' -exec grep -HIr -- '{prefix}' "'{}'" \;")
\end{lstlisting}
It is important to note that the local dir variable can be the root folder. In this case a special action takes place. Before the scan will start, a target\_root will be examined by the function get\_root\_diff().
This function returns a deviation Linux root file system. This deviation forms the target to scan on root level. The scan method remains the same, no matter if the scan at root level or elsewhere takes place.
During the search, the Linux find standard util searches for any kind of file (including hidden files) in the provided target order per iteration. The path builds the known overlay directory with the examined and prepared target folder, which is a merged union point.
Each file which is found will be piped into the grep command which uses the first dimension of the cartesian product, the RSA prefix. Because of the grep options the result is finally printed to the stdout buffer.

The scan\_for\_aws\_key(self) function doesn't includes a data\_structure to hold prefixes. Instead it only needs a working pattern. This regex pattern is created by analysing the structure of the AWS token, which is explained in the theoretical concept. From a programmatically perspective, the main difference is the very slim scan function, since there is nor for in for loop necessary. This is beneficial to the analysis time and the performace.
Because of the very similar implementation, nor further description is made at this point.

After the prototype has been described, it can be applied and tested. This will happen in the following evaluation chapter.
	

